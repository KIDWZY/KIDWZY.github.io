<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>TakumiWzy的博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-05-16T07:39:44.492Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>TakumiWzy</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>科技幻想篇【三】之全息影像AI</title>
    <link href="http://yoursite.com/2019/05/16/%E7%A7%91%E6%8A%80%E5%B9%BB%E6%83%B3%E7%AF%87%E3%80%90%E4%B8%89%E3%80%91%E4%B9%8B%E5%85%A8%E6%81%AF%E5%BD%B1%E5%83%8FAI/"/>
    <id>http://yoursite.com/2019/05/16/科技幻想篇【三】之全息影像AI/</id>
    <published>2019-05-16T04:35:04.000Z</published>
    <updated>2019-05-16T07:39:44.492Z</updated>
    
    <content type="html"><![CDATA[<h1 id="科技幻想篇全息影像AI之GateBox"><a href="#科技幻想篇全息影像AI之GateBox" class="headerlink" title="科技幻想篇全息影像AI之GateBox"></a>科技幻想篇全息影像AI之GateBox</h1><p>在之前的刀剑神域篇里面我还构想着有木有一款水晶柱形态的全息影像AI能够与人完成交互。就在最近两天我在网上冲浪的时候意外地发现了下面这款神奇的东西！！！…..にほんじん在脑洞和创新点子上这点还不得不佩服！</p><center> <img src="/images/gatebox6.gif"></center><p>是的，没错，这是名为<a href="https://baike.baidu.com/item/Gatebox" target="_blank" rel="noopener">Gatebox</a>的一款专为宅男定制的家用智能化全息机器人。源自Line（没错就是那个日本版微信）的一个下属业务公司Gatebox Inc.开发的。</p><h3 id="产品官网演示很炫酷："><a href="#产品官网演示很炫酷：" class="headerlink" title="产品官网演示很炫酷："></a>产品官网演示很炫酷：</h3><p><a href="https://gatebox.ai/home/" target="_blank" rel="noopener">https://gatebox.ai/home/</a></p><h3 id="B站开箱视频："><a href="#B站开箱视频：" class="headerlink" title="B站开箱视频："></a>B站开箱视频：</h3><p><a href="https://www.bilibili.com/video/av21011143?from=search&amp;seid=1305157800863921297" target="_blank" rel="noopener">https://www.bilibili.com/video/av21011143?from=search&amp;seid=1305157800863921297</a></p><p>根据我搜集到的信息，产品在2016年开始众筹，于2017年底开始只在日本本土发售限量300台！价格大概是：32万日元前后（约合人民币大约18950元），不得不说，宅男们的钱真好赚啊=_=！</p><h2 id="GateBox："><a href="#GateBox：" class="headerlink" title="GateBox："></a>GateBox：</h2><h3 id="AI人物逢妻光"><a href="#AI人物逢妻光" class="headerlink" title="AI人物逢妻光"></a>AI人物<a href="https://baike.baidu.com/item/逢妻光" target="_blank" rel="noopener">逢妻光</a></h3><center> <img src="/images/gatebox5.jpg" height="600" width="600"></center><h3 id="影像显示部分："><a href="#影像显示部分：" class="headerlink" title="影像显示部分："></a>影像显示部分：</h3><p>看完开箱视频之后，我发现她只是在中间构架了一块近似透明的显示屏，从正面看可以观察到人物的各种动作，但是从侧面看就是纸片人，没错，是的，就是一片平面屏幕。</p><center> <img src="/images/gatebox3.jpg"></center><p>细心地朋友可以发现其圆形底盘中间的一块透明屏幕。</p><p>说得严格一点，这根本不是全息影像。当然，人家卖家也没说这是全息人物2333333</p><h3 id="AI交互部分："><a href="#AI交互部分：" class="headerlink" title="AI交互部分："></a>AI交互部分：</h3><p>可以通过手机客户端进行聊天什么的，以及当作起床闹钟。看了开箱评论有人说就是一款豆浆机型闹钟哈哈。不过我觉得目前的技术水平上已经做得还算可以了。但是还是有很多改进的地方。</p><center> <img src="/images/gatebox1.jpg"></center><h2 id="HoloEra"><a href="#HoloEra" class="headerlink" title="HoloEra"></a>HoloEra</h2><p>在我国也有一款HoloEra全息投影AI好像比GateBox更加智能一些，而且利用到了四面投影技术，不过具体怎样还看大众的评价了。HE琥珀以最新的AI引擎与VR技术相结合，采用Intel高性能CPU，包括人脸识别、人体感应等在内的多模态识别系统，使之更具人性化、智能化。HE琥珀横跨AI、VR、Game。</p><p>然而，这个产品的全息技术就是利用机器顶部放置了一块9.7英寸分辨率为2048*1536的IPS屏幕，并将屏幕上的画面划分为四个三角形，分别显示在下方三角形玻璃物体的前后左右四个侧边，从而完成四个方向的立体影像。也不是真正意义上的全息投影。机器搭载了英特尔Z8350处理器，图像相当于来说比较流畅，某宝就有卖的。另外和GateBox一样，和手机App进行交互在目前阶段可能更方便，交互起来可能并不是那么智能。</p><center> <img src="/images/coper1.png"></center><h2 id="全息投影技术原理："><a href="#全息投影技术原理：" class="headerlink" title="全息投影技术原理："></a>全息投影技术原理：</h2><p><strong>全息投影</strong>，也称虚拟成像技术。利用光的干涉与衍射原理记录并再现物体真实的三维图像。真正的全息投影不仅360°可看，720°都是可看的。其第一步是利用干涉原理记录物体光波信息，此即拍摄过程；第二步是利用衍射原理再现物体光波信息，即成像过程。由此可见，真真的全息投影技术还没有真正被创造出来，市场上的技术只是无限接近。核心技术之一就是全息投影膜了。</p><p>后面简单说一下查到的一些全息投影的实现形式，不一定全面：</p><p><strong>空气投影和交互技术：</strong>它可以在气流形成的墙上投影出具有交互功能的图像。源自海市蜃楼的原理，将图像投射在水蒸气上，可以形成层次和立体感很强的图像。</p><p><strong>激光束投射实体的3D影像：</strong>利用氮气和氧气在空气中散开时，混合成的气体变成灼热的浆状物质，并在空气中形成一个短暂的3D图像。</p><p> <strong>360度全息显示屏：</strong>将图像投影在一种高速旋转的镜子上从而实现三维图像。</p><p><strong>边缘消隐技术：</strong>将画面投射到「全息」膜上或者反射到「全息」膜上，再利用暗场来隐藏起全息膜，从而形成图像悬浮在空中的效果。上面的HoloEra就是该方法。</p><p><strong>旋转LED显示技术：</strong>利用视觉暂留原理，通过LED的高速旋转来实现平面成像，但由于LED灯条在旋转时并非密不透风，观察者依然可以看到灯条后的物体，从而让观察者感觉画面悬浮在空中，实现类似3D的效果。</p><p>当我们需要判断看见的「全息」投影是否是真正的全息时，只需知道有无<a href="https://baike.baidu.com/item/全息图/11052216" target="_blank" rel="noopener">全息图</a>即可。</p><h3 id="简单手工伪全息投影："><a href="#简单手工伪全息投影：" class="headerlink" title="简单手工伪全息投影："></a>简单手工伪全息投影：</h3><p>透明塑料薄膜（用于衍射光反射成像）+全息投影片源（手机或平板播放）</p><center> <img src="/images/quanxi1.png" height="400" width="400"></center><center> <img src="/images/miku2.png"></center><p>由此可见这种简单的设计只是伪全息投影。具体制作参照流程<a href="https://jingyan.baidu.com/article/86f4a73e820d4b37d6526984.html#!/article/86f4a73e820d4b37d6526984" target="_blank" rel="noopener">百度</a>即可。</p><h4 id="效果如下："><a href="#效果如下：" class="headerlink" title="效果如下："></a>效果如下：</h4><center> <img src="/images/miku3.jpg"></center><p>有兴趣的可以自己尝试做一个玩一玩。</p><h2 id="关于全息影像AI展望："><a href="#关于全息影像AI展望：" class="headerlink" title="关于全息影像AI展望："></a>关于全息影像AI展望：</h2><p>首先，就像我之前在刀剑神域篇里面所探讨的AI产品一样。日本在科技创新和探索的精神还是值得我们学习的。其次，关于GateBox产品与HoloEra的展望，我之前就曾构想过这种类似的产品，没想到已经被人家都给做出来成品了，虽然里面所涉及的AI影像并不是真正意义上的全息，但还是可以朝着这个方向努力的。还有就是将当前的人工智能领域内的自然语言处理，计算机视觉，情感计算，知识图谱，还有就是当前比较火的GAN生成对抗网络，将AI可以进行自主学习并且增强自身的理解与学习水平一起集成到这么一款产品上，并且最大程度上降低成本，是我们需要不断努力才能看到进展的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;科技幻想篇全息影像AI之GateBox&quot;&gt;&lt;a href=&quot;#科技幻想篇全息影像AI之GateBox&quot; class=&quot;headerlink&quot; title=&quot;科技幻想篇全息影像AI之GateBox&quot;&gt;&lt;/a&gt;科技幻想篇全息影像AI之GateBox&lt;/h1&gt;&lt;p&gt;在之
      
    
    </summary>
    
    
      <category term="科技随想" scheme="http://yoursite.com/categories/%E7%A7%91%E6%8A%80%E9%9A%8F%E6%83%B3/"/>
    
    
      <category term="科技宅" scheme="http://yoursite.com/tags/%E7%A7%91%E6%8A%80%E5%AE%85/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫【三】静态html网页图片下载</title>
    <link href="http://yoursite.com/2019/05/12/Python%E7%88%AC%E8%99%AB%E3%80%90%E4%B8%89%E3%80%91%E9%9D%99%E6%80%81html%E7%BD%91%E9%A1%B5%E5%9B%BE%E7%89%87%E4%B8%8B%E8%BD%BD/"/>
    <id>http://yoursite.com/2019/05/12/Python爬虫【三】静态html网页图片下载/</id>
    <published>2019-05-12T03:52:30.000Z</published>
    <updated>2019-05-13T01:53:04.693Z</updated>
    
    <content type="html"><![CDATA[<p>前面说了两个简单的库requests和BeautifulSoup即可实现静态HTML网页的图片爬取。但是有些网页是动态加载的，例如是用js动态加载的，使用普通的方式抓取数据是找不到相关数据的，通常明明在浏览器里有相应的信息，但是python抓取的网页中缺少了对应的信息，这通常是网页使用的是js异步加载数据，在动态显示出来。</p><p>一种处理方式是找出相应的js接口，但是有时这是非常难得因为还的分析js的调用参数，而有些参数是有加密的，还的进行解密操作；另一种出来方式是python调用浏览器，控制浏览器返回相应的信息。</p><p>目前只进行静态HTML内容的下载与获取步骤的学习。</p><h1 id="图像文件保存"><a href="#图像文件保存" class="headerlink" title="图像文件保存"></a>图像文件保存</h1><h2 id="1-文件夹创建与切换"><a href="#1-文件夹创建与切换" class="headerlink" title="1.文件夹创建与切换"></a>1.文件夹创建与切换</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.makedirs(os.path.join(<span class="string">"F:\name"</span>, filename))</span><br><span class="line"><span class="comment">#在目录F:\name下创建名为filename的文件夹</span></span><br><span class="line">os.chdir(<span class="string">"F:\name\\"</span> + filename)</span><br><span class="line"><span class="comment">#切换工作路径到F:\name\filename下</span></span><br></pre></td></tr></table></figure><h2 id="2-图像文件保存"><a href="#2-图像文件保存" class="headerlink" title="2.图像文件保存"></a>2.图像文件保存</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">f = open(name+<span class="string">'.jpg'</span>, <span class="string">'wb'</span>)<span class="comment">##写入多媒体文件要以二进制参数b写入！</span></span><br><span class="line">f.write(img.content) <span class="comment">##多媒体文件要是用content！</span></span><br><span class="line">f.close()</span><br></pre></td></tr></table></figure><h1 id="实战：爬取网站动漫图片"><a href="#实战：爬取网站动漫图片" class="headerlink" title="实战：爬取网站动漫图片"></a>实战：爬取网站动漫图片</h1><p>这里给出一个动漫图片的网站大全，里面的动漫图片与萌妹纸图片很多：</p><p><a href="https://safebooru.donmai.us/posts?page=1" target="_blank" rel="noopener">https://safebooru.donmai.us/posts?page=1</a></p><p>给出一个简单可行的代码，目前来看的话，仅仅是在跑一些计算机视觉的程序时，这些下载的图片已经够用了。我的打算是后面如果有时间的话就学一下其他爬取动态网页的方式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Created on Sun May 12 12:28:03 2019</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">@author: Ziyuan</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment">#导入必要的模块</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup <span class="keyword">as</span> BS</span><br><span class="line"><span class="keyword">import</span> os </span><br><span class="line"></span><br><span class="line"><span class="comment">#定义一个爬取图片的类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImageCrawl</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">download</span><span class="params">(self,url,filename)</span>:</span></span><br><span class="line">        <span class="keyword">if</span>  <span class="keyword">not</span> os.path.exists(<span class="string">'pictures'</span>):</span><br><span class="line">            os.makedirs(<span class="string">'pictures'</span>)</span><br><span class="line">        r = requests.get(url,stream=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">with</span> open(filename,<span class="string">"wb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="keyword">for</span> image <span class="keyword">in</span> r.iter_content(chunk_size=<span class="number">1024</span>):</span><br><span class="line">                <span class="keyword">if</span> image:</span><br><span class="line">                    f.write(image)</span><br><span class="line">                    f.flush</span><br><span class="line">                        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getImage</span><span class="params">(self,start=<span class="number">1</span>,end=<span class="number">10</span>)</span>:</span></span><br><span class="line">        <span class="comment">#定义需要获取的图片的起始页面</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(start,end+<span class="number">1</span>):</span><br><span class="line">            <span class="comment">#浏览每一页地址可发现就最后一个字符不同</span></span><br><span class="line">            url = <span class="string">'https://safebooru.donmai.us/posts?page=&#123;&#125;'</span>.format(i)</span><br><span class="line">            html = requests.get(url,timeout=(<span class="number">3</span>,<span class="number">20</span>)).text</span><br><span class="line">            soup = BS(html,<span class="string">'lxml'</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> img <span class="keyword">in</span> soup.find_all(<span class="string">'img'</span>,class_=<span class="string">'has-cropped-true'</span>):</span><br><span class="line">                <span class="comment">#找到图片实际的网址</span></span><br><span class="line">                target_url = img[<span class="string">'src'</span>]</span><br><span class="line">                <span class="comment">#将图像路径格式保存为文件名</span></span><br><span class="line">                filename = os.path.join(<span class="string">'pictures'</span>,target_url.split(<span class="string">'/'</span>)[<span class="number">-1</span>])</span><br><span class="line">                self.download(target_url,filename)</span><br><span class="line">            print(<span class="string">"正在下载第&#123;&#125;页的图片......"</span>.format(i))</span><br><span class="line">            print(<span class="string">"总进度为&#123;&#125; %"</span>.format((i/end)*<span class="number">100</span>))</span><br><span class="line">                </span><br><span class="line">                </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line">    craw = ImageCrawl()</span><br><span class="line">    craw.getImage()</span><br></pre></td></tr></table></figure><p>运行结果如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">正在下载第<span class="number">1</span>页的图片......</span><br><span class="line">总进度为<span class="number">10.0</span> %</span><br><span class="line">正在下载第<span class="number">2</span>页的图片......</span><br><span class="line">总进度为<span class="number">20.0</span> %</span><br><span class="line">正在下载第<span class="number">3</span>页的图片......</span><br><span class="line">总进度为<span class="number">30.0</span> %</span><br><span class="line">正在下载第<span class="number">4</span>页的图片......</span><br><span class="line">总进度为<span class="number">40.0</span> %</span><br><span class="line">正在下载第<span class="number">5</span>页的图片......</span><br><span class="line">总进度为<span class="number">50.0</span> %</span><br><span class="line">正在下载第<span class="number">6</span>页的图片......</span><br><span class="line">总进度为<span class="number">60.0</span> %</span><br><span class="line">正在下载第<span class="number">7</span>页的图片......</span><br><span class="line">总进度为<span class="number">70.0</span> %</span><br><span class="line">正在下载第<span class="number">8</span>页的图片......</span><br><span class="line">总进度为<span class="number">80.0</span> %</span><br><span class="line">正在下载第<span class="number">9</span>页的图片......</span><br><span class="line">总进度为<span class="number">90.0</span> %</span><br><span class="line">正在下载第<span class="number">10</span>页的图片......</span><br><span class="line">总进度为<span class="number">100.0</span> %</span><br></pre></td></tr></table></figure><p>下载的图片文件如下：</p><center><img src="/images/anime1.PNG" alt></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;前面说了两个简单的库requests和BeautifulSoup即可实现静态HTML网页的图片爬取。但是有些网页是动态加载的，例如是用js动态加载的，使用普通的方式抓取数据是找不到相关数据的，通常明明在浏览器里有相应的信息，但是python抓取的网页中缺少了对应的信息，这通
      
    
    </summary>
    
    
      <category term="Python爬虫" scheme="http://yoursite.com/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="二次元" scheme="http://yoursite.com/tags/%E4%BA%8C%E6%AC%A1%E5%85%83/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫【二】BeautifulSoup库的使用</title>
    <link href="http://yoursite.com/2019/05/11/Python%E7%88%AC%E8%99%AB%E3%80%90%E4%BA%8C%E3%80%91BeautifulSoup%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    <id>http://yoursite.com/2019/05/11/Python爬虫【二】BeautifulSoup库的使用/</id>
    <published>2019-05-11T04:42:44.000Z</published>
    <updated>2019-05-12T03:51:26.027Z</updated>
    
    <content type="html"><![CDATA[<p>前面学习了requests库的用法之后，我们只能得到网站返回的响应，但是具体里面有哪些东西我们却不知道。<a href="http://www.crummy.com/software/BeautifulSoup/" target="_blank" rel="noopener">Beautiful Soup</a> 是一个可以从HTML或XML文件中提取数据的Python库。直观理解就是把网页解析成一锅好汤：），下面开始记录一下在爬取图像中所用到的一些操作。其他一些复杂的操作可根据自己的需求去其官方文档查询即可：</p><p><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/" target="_blank" rel="noopener">https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/</a></p><h1 id="BeautifulSoup库的使用方法"><a href="#BeautifulSoup库的使用方法" class="headerlink" title="BeautifulSoup库的使用方法"></a>BeautifulSoup库的使用方法</h1><h2 id="1-BeautifulSoup库的安装与导入"><a href="#1-BeautifulSoup库的安装与导入" class="headerlink" title="1.BeautifulSoup库的安装与导入"></a>1.BeautifulSoup库的安装与导入</h2><p>看了一下文档，说道现在的项目中使用Beautiful Soup 4, <a href="http://www.baidu.com/" target="_blank" rel="noopener">移植到BS4</a>。</p><p>因此安装方式如下：</p><p><code>pip install bs4</code></p><p>或者同理，使用conda命令安装也是：</p><p><code>conda install bs4</code></p><p>导入方式：</p><p><code>from bs4 import BeautifulSoup</code></p><h2 id="2-快速入门"><a href="#2-快速入门" class="headerlink" title="2.快速入门"></a>2.快速入门</h2><h3 id="2-1导入BeautifulSoup"><a href="#2-1导入BeautifulSoup" class="headerlink" title="2.1导入BeautifulSoup"></a>2.1导入BeautifulSoup</h3><p><code>from bs4 import BeautifulSoup</code></p><h3 id="2-2设置requests请求头headers"><a href="#2-2设置requests请求头headers" class="headerlink" title="2.2设置requests请求头headers"></a>2.2设置requests请求头headers</h3><p>（PS：这一步可以缺省，主要是看自己是否想隐藏自己的访问来源等参照requests库的用法）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">headers=&#123;<span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.110 Safari/537.36'</span>,<span class="string">'referer'</span>:<span class="string">"www.example.com"</span> &#125;</span><br><span class="line">all_url = <span class="string">'http://example.com/'</span> </span><br><span class="line"><span class="string">'User-Agent'</span>:请求方式  </span><br><span class="line"><span class="string">'referer'</span>:从哪个链接跳转进来的</span><br></pre></td></tr></table></figure><h3 id="2-3建立访问连接"><a href="#2-3建立访问连接" class="headerlink" title="2.3建立访问连接"></a>2.3建立访问连接</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置一个html参数获取返回的响应</span></span><br><span class="line">html = requests.get(all_url,  headers=headers)</span><br><span class="line">all_url：起始的地址，也就是访问的第一个页面</span><br><span class="line">headers：请求头，告诉服务器是谁来了。</span><br><span class="line">requests.get：该方法能获取all_url的页面内容并且返回内容。</span><br></pre></td></tr></table></figure><h3 id="2-4用BeautifulSoup解析获取的页面"><a href="#2-4用BeautifulSoup解析获取的页面" class="headerlink" title="2.4用BeautifulSoup解析获取的页面"></a>2.4用BeautifulSoup解析获取的页面</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Soup = BeautifulSoup(html.text, <span class="string">'lxml'</span>)</span><br><span class="line">BeautifulSoup：用于解析页面</span><br><span class="line">lxml：解析器</span><br><span class="line">html.text：页面的内容</span><br></pre></td></tr></table></figure><p>Beautiful Soup解析html时实际上依赖解析器，它除了支持Python标准库中的HTML解析器外，还支持一些第三方解析器（比如lxml）。如果没有安装这个的话，需要提前安装：<code>pip install lxml</code>，不然会报错！</p><p>这里简要说明一下不同的类别解析器：</p><table><thead><tr><th style="text-align:center">解析器</th><th style="text-align:center">使用方法</th><th style="text-align:center">优势</th><th style="text-align:center">劣势</th></tr></thead><tbody><tr><td style="text-align:center">Python标准库</td><td style="text-align:center"><code>BeautifulSoup(markup, &quot;html.parser&quot;)</code></td><td style="text-align:center">Python的内置标准库、执行速度适中、文档容错能力强</td><td style="text-align:center">Python 2.7.3及Python 3.2.2之前的版本文档容错能力差</td></tr><tr><td style="text-align:center">lxml HTML解析器</td><td style="text-align:center"><code>BeautifulSoup(markup, &quot;lxml&quot;)</code></td><td style="text-align:center">速度快、文档容错能力强</td><td style="text-align:center">需要安装C语言库</td></tr><tr><td style="text-align:center">lxml XML解析器</td><td style="text-align:center"><code>BeautifulSoup(markup, &quot;xml&quot;)</code></td><td style="text-align:center">速度快、唯一支持XML的解析器</td><td style="text-align:center">需要安装C语言库</td></tr><tr><td style="text-align:center">html5lib</td><td style="text-align:center"><code>BeautifulSoup(markup, &quot;html5lib&quot;)</code></td><td style="text-align:center">最好的容错性、以浏览器的方式解析文档、生成HTML5格式的文档</td><td style="text-align:center">速度慢、不依赖外部扩展</td></tr></tbody></table><p>通常在一般情况下，我都是使用的lxml来进行解析。用时只需修改第二个参数即可<code>BeautifulSoup(html.text, &#39;lxml&#39;)</code>。</p><h3 id="2-5进一步处理获取的页面内容"><a href="#2-5进一步处理获取的页面内容" class="headerlink" title="2.5进一步处理获取的页面内容"></a>2.5进一步处理获取的页面内容</h3><p>解析完页面后我们需要找到特定的内容具体包含了哪些内容或者图片的实际网址是什么，此时就需要根据BeautifulSoup返回的Soup中的实例中的一些方法来进行查找，这里面的方法就太多了，我只列举一下我常用到的：</p><h4 id="1-节点选择器"><a href="#1-节点选择器" class="headerlink" title="1.节点选择器"></a>1.节点选择器</h4><p>直接根据需要调用的结点的元素的名称，在利用其<code>string</code>属性就可以得到文本内容了。</p><h4 id="2-提取信息"><a href="#2-提取信息" class="headerlink" title="2.提取信息"></a>2.提取信息</h4><h5 id="2-1获取名称"><a href="#2-1获取名称" class="headerlink" title="2.1获取名称"></a>2.1获取名称</h5><p>利用结点内的一些属性直接调用即可。分为两步：①选取节点名，②调用的属性</p><p><code>print(soup.title.name)</code>即：选择title节点，调用name属性获得节点名称。</p><h5 id="2-2获取属性"><a href="#2-2获取属性" class="headerlink" title="2.2获取属性"></a>2.2获取属性</h5><p>一个节点可能具有多种属性如：id与class，选择节点后，调用<code>attrs</code>获取所有属性。</p><p><code>print(soup.p.attrs)</code></p><p><code>print(soup.p.attrs[&#39;name&#39;])</code></p><h5 id="2-3获取内容"><a href="#2-3获取内容" class="headerlink" title="2.3获取内容"></a>2.3获取内容</h5><p>利用节点的<code>string</code>属性获取节点元素内包含的文本信息内容。</p><h5 id="2-4获取目标内容"><a href="#2-4获取目标内容" class="headerlink" title="2.4获取目标内容"></a>2.4获取目标内容</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;a href =<span class="comment"># &gt;内容&lt;/a&gt;</span></span><br><span class="line">a[i]/get_text():获取第i个a标签里面的内容</span><br></pre></td></tr></table></figure><h4 id="3-方法选择器"><a href="#3-方法选择器" class="headerlink" title="3.方法选择器"></a>3.方法选择器</h4><p>通过属性来进行选择很直接，但是进行比较复杂的选择时就显得不够方便灵活。因此可以同通过BeautifulSoup中提供的查询方法：</p><p><code>find()</code>查找第一个并返回</p><p><code>find_all()</code>搜索当前tag的所有tag子节点,并判断是否符合过滤器的条件。</p><p>调用方式：<code>find_all(name , attrs , recursive , text , **kwargs)</code></p><p><strong>因为我常用的就是这两个，给出举例：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">all_a = Soup.find(<span class="string">'div'</span>, class_=<span class="string">'pic'</span>).find_all(<span class="string">'a'</span>)[<span class="number">-2</span>]</span><br><span class="line">Soup.find（）查找某一个</span><br><span class="line">find_all（）查找所有的，返回一个列表</span><br><span class="line">.find(<span class="string">'img'</span>)[<span class="string">'src'</span>]获取img的src链接属性    </span><br><span class="line">class__:获取目标的类名</span><br><span class="line">div/a:类型条件为div/a的</span><br><span class="line">[<span class="number">-2</span>]可以用来去掉最后多匹配的标签，这里表示去掉最后两个a标签</span><br></pre></td></tr></table></figure><p><code>find_all()</code> 和 <code>find()</code> 方法,Beautiful Soup中还有10个用于搜索的API.它们中的五个用的是与 <code>find_all()</code> 相同的搜索参数,另外5个与 <code>find()</code> 方法的搜索参数类似.区别仅是它们搜索文档的不同部分。</p><h6 id="name-参数¶"><a href="#name-参数¶" class="headerlink" title="name 参数¶"></a>name 参数<a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id32" target="_blank" rel="noopener">¶</a></h6><p><code>name</code> 参数可以查找所有名字为 <code>name</code> 的tag,字符串对象会被自动忽略掉.</p><p>简单的用法如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">soup.find_all(<span class="string">"title"</span>)</span><br><span class="line"><span class="comment"># [&lt;title&gt;The Dormouse's story&lt;/title&gt;]</span></span><br></pre></td></tr></table></figure><p>重申: 搜索 <code>name</code> 参数的值可以使任一类型的 <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id25" target="_blank" rel="noopener">过滤器</a> ,字符窜,正则表达式,列表,方法或是 <code>True</code> .</p><h6 id="keyword-参数"><a href="#keyword-参数" class="headerlink" title="keyword 参数"></a>keyword 参数</h6><p>如果一个指定名字的参数不是搜索内置的参数名,搜索时会把该参数当作指定名字tag的属性来搜索,如果包含一个名字为 <code>id</code> 的参数,Beautiful Soup会搜索每个tag的”id”属性.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">soup.find_all(id=<span class="string">'link2'</span>)</span><br><span class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;]</span></span><br></pre></td></tr></table></figure><p>如果传入 <code>href</code> 参数,Beautiful Soup会搜索每个tag的”href”属性:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">soup.find_all(href=re.compile(<span class="string">"elsie"</span>))</span><br><span class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;]</span></span><br></pre></td></tr></table></figure><p>搜索指定名字的属性时可以使用的参数值包括 <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id27" target="_blank" rel="noopener">字符串</a> , <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id28" target="_blank" rel="noopener">正则表达式</a> , <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#id29" target="_blank" rel="noopener">列表</a>, <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/#true" target="_blank" rel="noopener">True</a> .</p><p>下面的例子在文档树中查找所有包含 <code>id</code> 属性的tag,无论 <code>id</code> 的值是什么:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">soup.find_all(id=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;Elsie&lt;/a&gt;,</span></span><br><span class="line"><span class="comment">#  &lt;a class="sister" href="http://example.com/lacie" id="link2"&gt;Lacie&lt;/a&gt;,</span></span><br><span class="line"><span class="comment">#  &lt;a class="sister" href="http://example.com/tillie" id="link3"&gt;Tillie&lt;/a&gt;]</span></span><br></pre></td></tr></table></figure><p>使用多个指定名字的参数可以同时过滤tag的多个属性:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">soup.find_all(href=re.compile(<span class="string">"elsie"</span>), id=<span class="string">'link1'</span>)</span><br><span class="line"><span class="comment"># [&lt;a class="sister" href="http://example.com/elsie" id="link1"&gt;three&lt;/a&gt;]</span></span><br></pre></td></tr></table></figure><p>有些tag属性在搜索不能使用,比如HTML5中的 data-* 属性:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data_soup = BeautifulSoup(<span class="string">'&lt;div data-foo="value"&gt;foo!&lt;/div&gt;'</span>)</span><br><span class="line">data_soup.find_all(data-foo=<span class="string">"value"</span>)</span><br><span class="line"><span class="comment"># SyntaxError: keyword can't be an expression</span></span><br></pre></td></tr></table></figure><p>但是可以通过 <code>find_all()</code> 方法的 <code>attrs</code> 参数定义一个字典参数来搜索包含特殊属性的tag:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_soup.find_all(attrs=&#123;<span class="string">"data-foo"</span>: <span class="string">"value"</span>&#125;)</span><br><span class="line"><span class="comment"># [&lt;div data-foo="value"&gt;foo!&lt;/div&gt;]</span></span><br></pre></td></tr></table></figure><p>记住: <code>find_all()</code> 和 <code>find()</code> 只搜索当前节点的所有子节点,孙子节点等. <code>find_parents()</code> 和 <code>find_parent()</code> 用来搜索当前节点的父辈节点,搜索方法与普通tag的搜索方法相同,搜索文档搜索文档包含的内容。另外还有子孙节点，兄弟节点，父节点和祖先节点等。按照CSS类名搜索tag的功能非常实用,但标识CSS类名的关键字 <code>class</code> 在Python中是保留字,使用 <code>class</code> 做参数会导致语法错误.从Beautiful Soup的4.1.1版本开始,可以通过 <code>class_</code>参数搜索有指定CSS类名的tag。</p><h4 id="4-CSS样式选择器"><a href="#4-CSS样式选择器" class="headerlink" title="4.CSS样式选择器"></a>4.CSS样式选择器</h4><p>使用CSS选择器时，需要调用<code>select()</code>方法，传入想要的CSS选择即可。</p><p>具体内容可参考：参考<a href="http://www.w3school.com.cn/cssref/css_selectors.asp了解。" target="_blank" rel="noopener">http://www.w3school.com.cn/cssref/css_selectors.asp了解。</a></p><p>参考资料：</p><p><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/" target="_blank" rel="noopener">https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/</a></p><p><a href="https://cuiqingcai.com/5548.html" target="_blank" rel="noopener">https://cuiqingcai.com/5548.html</a></p><p><a href="https://segmentfault.com/a/1190000011192866" target="_blank" rel="noopener">https://segmentfault.com/a/1190000011192866</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;前面学习了requests库的用法之后，我们只能得到网站返回的响应，但是具体里面有哪些东西我们却不知道。&lt;a href=&quot;http://www.crummy.com/software/BeautifulSoup/&quot; target=&quot;_blank&quot; rel=&quot;noopener
      
    
    </summary>
    
    
      <category term="Python爬虫" scheme="http://yoursite.com/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="http://yoursite.com/tags/Python%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫【一】requests库的使用</title>
    <link href="http://yoursite.com/2019/05/11/Python%E7%88%AC%E8%99%AB%E3%80%90%E4%B8%80%E3%80%91requests%E5%BA%93%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
    <id>http://yoursite.com/2019/05/11/Python爬虫【一】requests库的使用/</id>
    <published>2019-05-11T04:40:07.000Z</published>
    <updated>2019-05-17T05:43:30.741Z</updated>
    
    <content type="html"><![CDATA[<p>前言：上周在学习VAE（变分自编码器）和DCGAN（深度生成对抗网络）用于生成人脸图像时，发现需要大量的图片数据。包括一些特殊的人脸，二次元动漫头像之类，想要制作自己理想的<del>喜欢的二次元妹纸图片</del> 数据集，因此发现利用python来对一些网站图片进行爬取是最方便的！因此，相当于同时扩展一下知识面，将所用到的爬取图片的库的使用方法结合其官方的使用文档做以记录。</p><h1 id="requests库的使用方法"><a href="#requests库的使用方法" class="headerlink" title="requests库的使用方法"></a>requests库的使用方法</h1><h2 id="1-requests库的安装与导入"><a href="#1-requests库的安装与导入" class="headerlink" title="1.requests库的安装与导入"></a>1.requests库的安装与导入</h2><p>windows用户电脑安装了Python后使用其包管理工具命令pip安装：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install requests</span><br></pre></td></tr></table></figure><p>同理，若用conda则安装命令为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install requests</span><br></pre></td></tr></table></figure><p>其他一些额外说明和安装方式可参考中文版官方文档：</p><p><a href="http://cn.python-requests.org/zh_CN/latest/" target="_blank" rel="noopener">http://cn.python-requests.org/zh_CN/latest/</a></p><p>导入requests包：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">r = requests.get(<span class="string">'https://www.baidu.com/'</span>)  <span class="comment"># 向网站发送请求</span></span><br><span class="line">print(r.status_code)  <span class="comment"># 返回状态码200</span></span><br></pre></td></tr></table></figure><p>常见状态码与其含义：</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">类别</th><th style="text-align:center">原因短语</th></tr></thead><tbody><tr><td style="text-align:center">1xx</td><td style="text-align:center">Informational(信息性状态码)</td><td style="text-align:center">接受的请求正在处理</td></tr><tr><td style="text-align:center">2xx</td><td style="text-align:center">Success(成功状态码)</td><td style="text-align:center">请求正常处理完毕</td></tr><tr><td style="text-align:center">3xx</td><td style="text-align:center">Redirection(重定向状态码)</td><td style="text-align:center">需要进行附加操作以完成请求</td></tr><tr><td style="text-align:center">4xx</td><td style="text-align:center">Client Error(客户端错误状态码)</td><td style="text-align:center">服务器无法处理请求</td></tr><tr><td style="text-align:center">5xx</td><td style="text-align:center">Server Error(服务器错误状态码)</td><td style="text-align:center">服务器处理请求出错</td></tr></tbody></table><h2 id="2-为什么用requests？"><a href="#2-为什么用requests？" class="headerlink" title="2.为什么用requests？"></a>2.为什么用requests？</h2><p>requests是python的一种HTTP客户端库，与urllib,urllib2相似，虽然， python的标准库urllib2提供了大部分需要的HTTP功能，但是API的使用方法太逆天了，一个简单的功能就需要一大堆代码。因此选择requests更加方便省事。</p><h2 id="3-快速入门"><a href="#3-快速入门" class="headerlink" title="3.快速入门"></a>3.快速入门</h2><h3 id="3-1发送请求"><a href="#3-1发送请求" class="headerlink" title="3.1发送请求"></a>3.1发送请求</h3><p>调用方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">requests.get(url, params=<span class="literal">None</span>, **kwargs)</span><br><span class="line">说明：Sends a GET request.</span><br><span class="line">参数：url:简单理解为网站资源定位器。</span><br><span class="line">返回：<span class="keyword">return</span>: :<span class="class"><span class="keyword">class</span>:</span>`Response &lt;Response&gt;` object</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#首先将requests包导入</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="comment">#填写一个网站，使用r来接受返回的响应</span></span><br><span class="line">r = requests.get(<span class="string">'https://www.baidu.com/'</span>)  <span class="comment"># 向网站发送请求</span></span><br><span class="line"><span class="comment">#打印状态码</span></span><br><span class="line">print(r.status_code)  <span class="comment"># 返回状态码200</span></span><br></pre></td></tr></table></figure><p>到这里就可以使用参数r的各种方法与函数。</p><p>由于HTTP请求还有许多其他类型，例如：post,put,delet,head,options，都可以参考上面的方式实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#填写一个网站，使用r来接受返回的响应</span></span><br><span class="line">r = requests.post(<span class="string">'https://www.baidu.com/'</span>) </span><br><span class="line">r = requests.put(<span class="string">'https://www.baidu.com/'</span>)  </span><br><span class="line">r = requests.delet(<span class="string">'https://www.baidu.com/'</span>)  </span><br><span class="line">r = requests.head(<span class="string">'https://www.baidu.com/'</span>)  </span><br><span class="line">r = requests.option(<span class="string">'https://www.baidu.com/'</span>)</span><br></pre></td></tr></table></figure><p>最常用的就是get，其他的具体用到再去查询即可。</p><h3 id="3-2给url传递参数"><a href="#3-2给url传递参数" class="headerlink" title="3.2给url传递参数"></a>3.2给url传递参数</h3><p>即在某些url中有些特定的想查询的数据，是以键/值对形式放置在url中，具体是在一个问号的后面：<code>httpbin.org/get?key=val</code>，那么此时就可以自己设置以字典形式的关键字参数对这些参数进行传入。给出官方例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">payload = &#123;<span class="string">'key1'</span>: <span class="string">'value1'</span>, <span class="string">'key2'</span>: <span class="string">'value2'</span>&#125;</span><br><span class="line">r = requests.get(<span class="string">"http://httpbin.org/get"</span>, params=payload)</span><br><span class="line">print(r.url)<span class="comment">#打印出新的url可见自己设置的关键字参数以及被编码为新的网址</span></span><br><span class="line"><span class="comment">#输出：http://httpbin.org/get?key1=value1&amp;key2=value2</span></span><br></pre></td></tr></table></figure><h3 id="3-3获取响应内容"><a href="#3-3获取响应内容" class="headerlink" title="3.3获取响应内容"></a>3.3获取响应内容</h3><p><code>r.text</code>：unicode 字符集都能被无缝地解码。</p><p><code>r.content</code>：对于非文本请求，以字节方式获取二进制响应内容。</p><p><code>r.encoding</code>：返回编码方式</p><p><code>r.json</code>：解码JSON响应内容。</p><p>实例访问Bilibili：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">r = requests.get(<span class="string">'https://www.bilibili.com/'</span>)  <span class="comment"># 向网站发送请求,此时若</span></span><br><span class="line">print(r.status_code)  <span class="comment"># 返回状态码200</span></span><br><span class="line">print(r.text)</span><br><span class="line"><span class="comment">#输出&lt;!DOCTYPE html&gt;&lt;html lang="zh-Hans"&gt;&lt;head&gt;</span></span><br><span class="line"><span class="comment">#&lt;meta charset="utf-8"&gt;&lt;title&gt;哔哩哔哩 (゜-゜)つロ 干杯~-bilibili&lt;/title&gt;&lt;meta name=....省略....</span></span><br><span class="line">print(r.encoding)<span class="comment">#输出：utf-8</span></span><br></pre></td></tr></table></figure><p><code>r.raw</code>：获取来自服务器的原始套接字响应。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="comment">#使用r.raw方式，此时注意stream=True要设置True</span></span><br><span class="line">r = requests.get(<span class="string">'https://www.bilibili.com/'</span>,stream=<span class="literal">True</span>)  <span class="comment"># 向网站发送请求</span></span><br><span class="line">print(r.status_code)  <span class="comment"># 返回状态码200</span></span><br><span class="line">print(r.raw)<span class="comment">#输出：&lt;urllib3.response.HTTPResponse object at 0x000001BA63BEFEF0&gt;</span></span><br><span class="line">print(r.raw.read(<span class="number">6</span>))</span><br><span class="line"><span class="comment">#输出：</span></span><br><span class="line"><span class="number">200</span></span><br><span class="line">&lt;urllib3.response.HTTPResponse object at <span class="number">0x000001BA63C1AA20</span>&gt;</span><br><span class="line"><span class="string">b'\x1f\x8b\x08\x00\x00\x00'</span></span><br></pre></td></tr></table></figure><p>一般情况下，应该以<code>r.iter_content(chunk_size)</code>模式将文本流保存到文件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(filename, <span class="string">'wb'</span>) <span class="keyword">as</span> fd:</span><br><span class="line">    <span class="keyword">for</span> chunk <span class="keyword">in</span> r.iter_content(chunk_size):</span><br><span class="line">        fd.write(chunk)</span><br></pre></td></tr></table></figure><h3 id="3-4定制请求头部内容"><a href="#3-4定制请求头部内容" class="headerlink" title="3.4定制请求头部内容"></a>3.4定制请求头部内容</h3><p>首先通过<code>r.headers</code>来获取响应头内容。并且结果是以字典的形式返回了全部内容，我们也可以访问部分内容。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">'https://www.bilibili.com/'</span></span><br><span class="line">r = requests.get(url)</span><br><span class="line"><span class="comment">#打印信息</span></span><br><span class="line">print(r.request.headers[<span class="string">'User-Agent'</span>])</span><br><span class="line"><span class="comment">#输出：</span></span><br><span class="line">python-requests/<span class="number">2.21</span><span class="number">.0</span></span><br></pre></td></tr></table></figure><p>伪装请求头部是采集时经常用的，我们可以用这个方法来隐藏：</p><p>为请求添加 HTTP 头部，只要简单地传递一个 <code>dict</code> 给 <code>headers</code> 参数就可以了。注意: 所有的 header 值必须是 string、bytestring 或者 unicode。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">'https://www.bilibili.com/'</span></span><br><span class="line"><span class="comment">#自己定义一个User-Agent头部：abcdefg123</span></span><br><span class="line">headers = &#123;<span class="string">'User-Agent'</span>: <span class="string">'abcdefg123'</span>&#125;</span><br><span class="line"><span class="comment">#将头部信息传入url参数</span></span><br><span class="line">r = requests.get(url,headers=headers)</span><br><span class="line"><span class="comment">#打印信息</span></span><br><span class="line">print(r.request.headers[<span class="string">'User-Agent'</span>])</span><br><span class="line"><span class="comment">#输出：</span></span><br><span class="line">abcdefg123</span><br></pre></td></tr></table></figure><h3 id="3-5设置超时时间"><a href="#3-5设置超时时间" class="headerlink" title="3.5设置超时时间"></a>3.5设置超时时间</h3><p>通常我们在爬取网站时，网站响应时间过长影响我们后续操作。因此可以通过<code>timeout</code>属性设置超时时间，一旦超过这个时间还没获得响应内容，就会提示错误。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line">url = <span class="string">'https://www.bilibili.com/'</span></span><br><span class="line">r = requests.get(url,timeout=<span class="number">0.000001</span>)</span><br><span class="line"><span class="comment">#输出报错信息：</span></span><br><span class="line">ConnectTimeout: HTTPSConnectionPool(host=<span class="string">'www.bilibili.com'</span>, port=<span class="number">443</span>):....省略....</span><br></pre></td></tr></table></figure><h3 id="3-6代理访问"><a href="#3-6代理访问" class="headerlink" title="3.6代理访问"></a>3.6代理访问</h3><p>采集网站数据时，为避免被封IP，可以使用设置代理。requests也有相应的<code>proxies</code>属性。具体根据操作来进行设置网站。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">proxies = &#123;</span><br><span class="line">  <span class="string">"http"</span>: <span class="string">"http://xxxxxxxxxxxxxxx"</span>,</span><br><span class="line">  <span class="string">"https"</span>: <span class="string">"http://xxxxxxxxxxxxxxx"</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">r=requests.get(<span class="string">"http://www.bilibili.com"</span>, proxies=proxies)</span><br><span class="line">print(r.status_code)</span><br><span class="line">如果代理需要登录账户与密码：</span><br><span class="line">proxies = &#123;</span><br><span class="line">    <span class="string">"http"</span>: <span class="string">"http://user:pass@xxxxxxxxxxx/"</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>具体用时再去查。</p><h3 id="3-7访问Cookie"><a href="#3-7访问Cookie" class="headerlink" title="3.7访问Cookie"></a>3.7访问Cookie</h3><p>如果某个响应中包含一些 cookie，你可以快速访问它们：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">'http://example.com/some/cookie/setting/url'</span></span><br><span class="line">r = requests.get(url)</span><br><span class="line">print(r.cookies[<span class="string">'example_cookie_name'</span>])</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line"><span class="string">'example_cookie_value'</span></span><br></pre></td></tr></table></figure><p>发送你的cookies到服务器，可以使用 <code>cookies</code> 参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">'http://httpbin.org/cookies'</span></span><br><span class="line">cookies = dict(cookies_are=<span class="string">'working'</span>)</span><br><span class="line">r = requests.get(url, cookies=cookies)</span><br><span class="line">print(r.text)</span><br><span class="line"><span class="comment">#输出</span></span><br><span class="line"><span class="string">'&#123;"cookies": &#123;"cookies_are": "working"&#125;&#125;'</span></span><br></pre></td></tr></table></figure><p>其实在真正用到的时候只有获取其网站请求响应就以及足够了，具体其他一些额外的高级的用法用时候结合<a href="http://cn.python-requests.org/zh_CN/latest/" target="_blank" rel="noopener">官方文档</a>再去查询。后面结合BeautifulSoup包来对网站请求的响应来进行解析也很重要。</p><h2 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h2><p><a href="http://cn.python-requests.org/zh_CN/latest/" target="_blank" rel="noopener">http://cn.python-requests.org/zh_CN/latest/</a></p><p><a href="https://www.cnblogs.com/derek1184405959/p/8448875.html" target="_blank" rel="noopener">https://www.cnblogs.com/derek1184405959/p/8448875.html</a></p><p><a href="https://www.cnblogs.com/lgh344902118/p/6780960.html" target="_blank" rel="noopener">https://www.cnblogs.com/lgh344902118/p/6780960.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;前言：上周在学习VAE（变分自编码器）和DCGAN（深度生成对抗网络）用于生成人脸图像时，发现需要大量的图片数据。包括一些特殊的人脸，二次元动漫头像之类，想要制作自己理想的&lt;del&gt;喜欢的二次元妹纸图片&lt;/del&gt; 数据集，因此发现利用python来对一些网站图片进行爬取是
      
    
    </summary>
    
    
      <category term="Python爬虫" scheme="http://yoursite.com/categories/Python%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="Python爬虫" scheme="http://yoursite.com/tags/Python%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>复仇者联盟四观影感</title>
    <link href="http://yoursite.com/2019/05/01/%E5%A4%8D%E4%BB%87%E8%80%85%E8%81%94%E7%9B%9F%E5%9B%9B%E8%A7%82%E5%BD%B1%E6%84%9F/"/>
    <id>http://yoursite.com/2019/05/01/复仇者联盟四观影感/</id>
    <published>2019-05-01T02:04:25.000Z</published>
    <updated>2019-05-01T11:05:12.211Z</updated>
    
    <content type="html"><![CDATA[<h1 id="复仇者联盟四之终局之战观影感"><a href="#复仇者联盟四之终局之战观影感" class="headerlink" title="复仇者联盟四之终局之战观影感"></a><center>复仇者联盟四之终局之战观影感</center></h1><p>上周六抽出了一天时间与朋友去四道口附近的深影国际影城看了中国巨幕(CGS)的复联四，话说还是来北京快一年以来第一次出去看电影：）。之前一直不知道IMAX和巨幕电影之间的区别与联系，观影之前我还特意查了一下，喜欢看电影的朋友在购买电影票的时候，一般都会看到“imax厅”、“杜比全景声厅”、“杜比影院”、“中国巨幕”等等一些代表影厅配置的标签，但是这几大影厅之间的区别，还是有很多人觉得难以理解，唯一的感触可能就是“大”、“超大”、“更大”的直观感体验。</p><h2 id="那么这些特意分门别类的影厅之间到底有什么区别呢？"><a href="#那么这些特意分门别类的影厅之间到底有什么区别呢？" class="headerlink" title="那么这些特意分门别类的影厅之间到底有什么区别呢？"></a>那么这些特意分门别类的影厅之间到底有什么区别呢？</h2><p><strong>IMAX：</strong>其实是一家公司制定的放映标准系统。<strong>巨幕电影</strong>（<strong>IMAX</strong>，即<strong>Image Maximum</strong>的缩写）是一种能够放映比传统胶片更大和更高解像度的电影放映系统。整套系统包括以IMAX规格摄制的影片拷贝、放映机、音响系统、银幕等。标准的IMAX银幕为22米宽、16米高，但完全可以在更大的银幕播放，而且迄今为止不断有更大的IMAX银幕出现。</p><p><strong>中国巨幕：</strong>可以理解为国产的IMAX，是我们国家自主研发的一套电影放映系统，因此和IMAX差不太多，当然各项指标可能不及IMAX，比如清晰度什么的，IMAX相当于达到4K的清晰度，中国巨幕一般还是2K左右。不过没关系，我相信未来随着技术逐步完善，是可以达到世界水准的。</p><p><strong>【4K】：</strong>指的是<strong>4096×2160</strong>的像素分辨率，意思就是特别清楚，超高清的意思。当然现在还有一些衍生的分辨率也叫4K，比如说3840×2160。我们常说的1080P也是指的分辨率，相当于1920×1080，而2K则是2048×1080，所以这里的K就是像素的单位，1K=1024。所以看4K电影，就可以看到885万像素的高清晰画面，甚至能看清演员的鼻毛！！！</p><p>其实观影体验还算不错，因为买的票比较晚，好位置已经被抢完了，我们就挑了一个靠后排的边边上，除了屏幕稍微有点斜之外，其他还算可以。因为已经在首映过后三四天没逛知乎等了，，，再不去看就要被剧透完了。。。</p><p>回归正题，简单说一说复联4的观影感受吧，首先，时间长达3个小时左右，中间两个小时左右的时候，电影院的灯居然亮了23333，我严重怀疑灯是定时好的，，，还好过了一会儿又灭了不影响观影=_=！整体感觉复联四的节奏比较快，好多情节也是纠缠在一起，一开始讲到让穿越时间的时候我还是拒绝的，看着看着，唔姆，真香：）导致当时看起来还不错，但是回来之后发现剧情BUG还是有好多。不过呢，可能涉及到几个主要演员的签约时间到期了，导演这样考虑肯能也是有很多原因的。其实我最先看的复联第一部电影不是钢铁侠，而是美国队长，然后就一口气把复联系列电影给补完，我还看了美剧《神盾局特工》。总而言之呢，给我最初从追的复联系列画上了一个句号吧。虽然看到网上的正面负面评价都有许多，但是我觉得，目前人类的对时间与空间的认知上除了按照那样安排剧情走向，也很难创造出更有超前时空观念的科学背景意义的剧情了吧，同时我觉得这种安排也算给观众一个相对可以接受的结局了。</p><p>且行且珍惜，永怀初心，期待漫威的后续电影！！！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;复仇者联盟四之终局之战观影感&quot;&gt;&lt;a href=&quot;#复仇者联盟四之终局之战观影感&quot; class=&quot;headerlink&quot; title=&quot;复仇者联盟四之终局之战观影感&quot;&gt;&lt;/a&gt;&lt;center&gt;复仇者联盟四之终局之战观影感&lt;/center&gt;&lt;/h1&gt;&lt;p&gt;上周六抽出
      
    
    </summary>
    
    
    
      <category term="电影杂谈" scheme="http://yoursite.com/tags/%E7%94%B5%E5%BD%B1%E6%9D%82%E8%B0%88/"/>
    
  </entry>
  
  <entry>
    <title>本周小结</title>
    <link href="http://yoursite.com/2019/04/20/%E6%9C%AC%E5%91%A8%E5%B0%8F%E7%BB%93/"/>
    <id>http://yoursite.com/2019/04/20/本周小结/</id>
    <published>2019-04-20T15:27:18.000Z</published>
    <updated>2019-04-20T15:40:07.190Z</updated>
    
    <content type="html"><![CDATA[<p>每天都要进步一点点，如同AdBoost一般才能不断往前；总想着一口吃个胖子，什么都想学，什么都想速成，到头来造成的结果就是什么都没弄明白。</p><p>本周感觉自己的一些预期进度都十分缓慢，比如本来打算将cs231n的svm图像分类部分完成，并对之前FCN中后半部分遗留的一些没弄懂的问题给搞明白，事实上两样都没全部完成。只能说计划赶不上变化，本来一开始对SVM的知识部分在上学期学的时候就只听老师讲，什么最大间隔，软间隔，最优化之类，也就顶多停留在了记一记概念的考试水平，没有真正明白其具体的使用方法。直到在cs231n中遇到了svm的图像分类，才去MOOC重新将其内容看了一遍，然后就一直在看视频，cs231n的项目只是将代码给执行了一遍，并没有时间将其具体实现原理搞懂。然后，就陷入了缓慢的过程，后面时间又将python中的opencv的用法给简单过了一遍，觉得和C++中的用法还是有一些差异的，毕竟python中将其与numpy结合起来用时是真的很方便，不像C++中还需要预先给定义一个Mat数据类。</p><p>最后还是希望能够多总结多思考，本周对机器学习中的一些内容重新复习中发现，在课堂中学习到的东西不对其进行实际的应用操作的话，真的是又还给老师了：）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;每天都要进步一点点，如同AdBoost一般才能不断往前；总想着一口吃个胖子，什么都想学，什么都想速成，到头来造成的结果就是什么都没弄明白。&lt;/p&gt;
&lt;p&gt;本周感觉自己的一些预期进度都十分缓慢，比如本来打算将cs231n的svm图像分类部分完成，并对之前FCN中后半部分遗留的
      
    
    </summary>
    
    
    
      <category term="科研感悟" scheme="http://yoursite.com/tags/%E7%A7%91%E7%A0%94%E6%84%9F%E6%82%9F/"/>
    
  </entry>
  
  <entry>
    <title>科技幻想篇之刀剑神域【二】</title>
    <link href="http://yoursite.com/2019/04/14/%E7%A7%91%E6%8A%80%E5%B9%BB%E6%83%B3%E7%AF%87%E4%B9%8B%E5%88%80%E5%89%91%E7%A5%9E%E5%9F%9F%E3%80%90%E4%BA%8C%E3%80%91/"/>
    <id>http://yoursite.com/2019/04/14/科技幻想篇之刀剑神域【二】/</id>
    <published>2019-04-14T03:16:07.000Z</published>
    <updated>2019-05-12T04:06:26.626Z</updated>
    
    <content type="html"><![CDATA[<center>科技生活幻想之刀剑神域【二】</center><p>又到周末，可以写一写生活和感悟了。自己一直想对刀剑神域里面出现的，涉及到一些AI与AR及VR的系列设备进行一个总结，之前简要写了一些关于刀剑神域的感想，这一篇就主要介绍一下刀剑神域第一季SAO（Sword Art Online）和飞行系VR游戏 “ALO（Alfheim Online）”，以及第二季桐人受菊冈诚二郎之托进入<a href="https://baike.baidu.com/item/FPS/73269" target="_blank" rel="noopener">FPS</a>型VR游戏“GGO（<a href="https://baike.baidu.com/item/Gun%20Gale%20Online" target="_blank" rel="noopener">Gun Gale Online</a>）”中调查的“幽灵子弹篇”事件，还有就是剧场版刀剑神域之序列之争（Ordinal Scale）出现的AI进行简要总结和分析。</p><h3 id="VR虚拟实境技术（Virtual-Reality，简称VR）"><a href="#VR虚拟实境技术（Virtual-Reality，简称VR）" class="headerlink" title="VR虚拟实境技术（Virtual Reality，简称VR）"></a><center>VR虚拟实境技术（Virtual Reality，简称VR）</center></h3><h4 id="1，第一世代机"><a href="#1，第一世代机" class="headerlink" title="1，第一世代机"></a>1，第一世代机</h4><p>只有极少数娱乐场所和舒压机构有引进的最早期机器，作品中没有描述其名称。</p><h4 id="2，NERvGear"><a href="#2，NERvGear" class="headerlink" title="2，NERvGear"></a>2，NERvGear</h4><p>全名为“NERve Direct Linkage Environment System”</p><p>由埋藏在机器内的无数信号原件产生多重电场，和使用者的脑部直接连接，不是透过眼睛或耳朵等感觉器官，而是直接对脑部传送虚拟的五感情报来生成虚拟空间。同时透过回收脑部发给身体的电子讯号，就算在虚拟空间内进行各种运动，现实世界的身体也毫无反应。</p><center><img src="/images/NERvGear1.png" alt="NERvGear"></center><center><img src="/images/sao3.png" alt></center><p>设备连入时的画面。</p><p> 第一款民生用NERDLES机器，该设备为头戴式第一代民用完全潜行机器。设计者为茅场晶彦，由ARGUS公司以世上最早使用NERDLES技术的家用游戏机名义发售，在SAO事件发生之后被废弃。</p><center><img src="/images/NERvGear2.png" alt></center><h4 id="3，AmuSphere"><a href="#3，AmuSphere" class="headerlink" title="3，AmuSphere"></a>3，<strong>AmuSphere</strong></h4><center><img src="/images/AmuSphere1.png" alt></center><p><strong>AmuSphere</strong> NERvGear的后继机，由RECT公司贩售。外型为两个圆环并排的冠状器具。标榜“绝对安全”，是将NERvGear的保护系统与安全机构强化，电磁脉冲减弱后的产品。因为其他规格都和NERvGear相同，所以两部机器间游戏可以互通。</p><h4 id="4，第三世代机"><a href="#4，第三世代机" class="headerlink" title="4，第三世代机"></a>4，第三世代机</h4><p><strong>Medicuboid</strong> 医疗用完全潜行机器，名称由来为医疗（medical）和方块（cuboid）的组合字。NERvGear的延伸型，基础设计出自神代凛子。其最受注目的功能之一是临终关怀。</p><h3 id="增强现实技术AR（Augmented-Reality，简称-AR）"><a href="#增强现实技术AR（Augmented-Reality，简称-AR）" class="headerlink" title="增强现实技术AR（Augmented Reality，简称 AR）"></a><center>增强现实技术AR（Augmented Reality，简称 AR）</center></h3><h4 id="1，AUGMA"><a href="#1，AUGMA" class="headerlink" title="1，AUGMA"></a>1，AUGMA</h4><center><img src="/images/AUGMA1.png" alt></center><p>一个类似与后戴式耳机和眼镜类似的AR+VR设备，带上该装备之后将眼前的周围现实景化。从而达到实现周围场景交互，聊天，游戏，以及种种用途。</p><h4 id="2，深度学习技术和大数据推荐系统："><a href="#2，深度学习技术和大数据推荐系统：" class="headerlink" title="2，深度学习技术和大数据推荐系统："></a>2，深度学习技术和大数据推荐系统：</h4><center><img src="/images/AUGMA5.png" alt></center><center><img src="/images/AUGMA6.png" alt></center><p>根据个人喜好大数据进行了解饮食习惯进而推荐用餐等。</p><center><img src="/images/AUGMA2.png" alt></center><h4 id="3，实时交流："><a href="#3，实时交流：" class="headerlink" title="3，实时交流："></a>3，实时交流：</h4><p>和周围朋友进行游戏交互等无需依赖硬件设备。</p><center><img src="/images/AUGMA4.png" alt></center><p>和朋友进行实时聊天：</p><center><img src="/images/AUGMA8.png" alt></center><h4 id="4，健康和饮食安全掌控："><a href="#4，健康和饮食安全掌控：" class="headerlink" title="4，健康和饮食安全掌控："></a>4，健康和饮食安全掌控：</h4><p>以及根据个人饮食情况进而推断和建议健康的饮食计划：</p><center><img src="/images/AUGMA7.png" alt="提示卡路里热量过多"></center><h4 id="5，娱乐与游戏："><a href="#5，娱乐与游戏：" class="headerlink" title="5，娱乐与游戏："></a>5，娱乐与游戏：</h4><p>对周围建筑和三维场景进行实时模拟建模，利用AR和VR进行游戏操作等：</p><center><img src="/images/AUGMA10.png" alt="周围场景建筑虚拟化"></center><center><img src="/images/AUGMA11.png" alt></center><h3 id="人工智能角色"><a href="#人工智能角色" class="headerlink" title="人工智能角色"></a><center>人工智能角色</center></h3><h4 id="1，歌姬尤娜"><a href="#1，歌姬尤娜" class="headerlink" title="1，歌姬尤娜"></a>1，歌姬尤娜</h4><center><img src="/images/una2.jpg" alt></center><h4 id="2，摇光系统"><a href="#2，摇光系统" class="headerlink" title="2，摇光系统"></a>2，摇光系统</h4><p>“Soul translation”和“Fluctuating Light”，简称STL和摇光。</p><p>根据小说剧情貌似是赋予了爱丽丝机械身体之后进入了现实社会：</p><center><img src="/images/alice1.jpg" alt></center><p>整部作品可谓是五彩斑斓，里面的智能化程度非常之高，就目前科技水平来讲实现起来是非常困难的。</p><p>总之，目前先简要总结这么多，这里面涉及到的和自己感兴趣的领域诸如：深度学习技术，增强现实技术，虚拟显示技术，以及涉及到的基于大数据的推荐和搜索算法，神经智能脑科学等，都是很耐人寻味的，至于是否能够在将来的生活中见到刀剑神域中这种我认为已经是超智能的小巧便携式设备，仍然是一个未知数。不过让我们怀着对未知的向往和热爱，不断前行吧！</p><center><img src="/images/sao2.png" alt></center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;科技生活幻想之刀剑神域【二】&lt;/center&gt;

&lt;p&gt;又到周末，可以写一写生活和感悟了。自己一直想对刀剑神域里面出现的，涉及到一些AI与AR及VR的系列设备进行一个总结，之前简要写了一些关于刀剑神域的感想，这一篇就主要介绍一下刀剑神域第一季SAO（Sword A
      
    
    </summary>
    
    
      <category term="科技随想" scheme="http://yoursite.com/categories/%E7%A7%91%E6%8A%80%E9%9A%8F%E6%83%B3/"/>
    
    
      <category term="生活感悟" scheme="http://yoursite.com/tags/%E7%94%9F%E6%B4%BB%E6%84%9F%E6%82%9F/"/>
    
  </entry>
  
  <entry>
    <title>Numpy与KNN学习小结</title>
    <link href="http://yoursite.com/2019/04/14/Numpy%E4%B8%8EKNN%E5%AD%A6%E4%B9%A0%E5%B0%8F%E7%BB%93/"/>
    <id>http://yoursite.com/2019/04/14/Numpy与KNN学习小结/</id>
    <published>2019-04-14T02:27:45.000Z</published>
    <updated>2019-04-17T01:29:50.827Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Numpy与KNN学习小结"><a href="#Numpy与KNN学习小结" class="headerlink" title="Numpy与KNN学习小结"></a>Numpy与KNN学习小结</h1><p>本周主要借着对cs231n课程作业的对Cifar10数据集的KNN分类，学习和补充了一些numpy中的知识盲点，总结来看Numpy真的是很强大了，它里面主要依靠数组为核心，将一维向量，二维矩阵，三维甚至是高维的数据都以numpy数组类型来进行管理，有点像Tensorflow中的任何数据对象都是n维数组的张量意思。并且Numpy中的数据操作加减乘除都非常强大和方便，另外还有Fancy indexing，索引数组的形式多样化。还是需要多加实际操作才能真正熟练掌握常见用法。</p><h2 id="简要总结一下Numpy中的一些矩阵以及数组的操作："><a href="#简要总结一下Numpy中的一些矩阵以及数组的操作：" class="headerlink" title="简要总结一下Numpy中的一些矩阵以及数组的操作："></a>简要总结一下Numpy中的一些矩阵以及数组的操作：</h2><p>①数组的合并，分割</p><p>②矩阵运算</p><p>③聚合运算：np.sum,np.max,np.min即一组值变为一个值。</p><p>④arg运算</p><p>⑤numpy中的比较以及Fancy indexing</p><ul><li>注意：linspace语句创建线性等分间隔的空间，从0到20（可取到，<em>而python中range,与np.arange()是取不到的，即左闭右开区间</em>），等分11个数。</li></ul><blockquote><p>np.linspace(0,20,11)<br>array([ 0.,  2.,  4.,  6.,  8., 10., 12., 14., 16., 18., 20.])</p></blockquote><blockquote><p>np.arange(20)<br>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,17, 18, 19])</p></blockquote><h2 id="KNN分类的思路进行梳理："><a href="#KNN分类的思路进行梳理：" class="headerlink" title="KNN分类的思路进行梳理："></a>KNN分类的思路进行梳理：</h2><p>依照cifar10的案例进行总结：</p><p>数据分类训练集train,和测试集test，在交叉验证时将训练集进行split，一部分当作训练集，一部分当作测试集。</p><h3 id="主要分为四部分："><a href="#主要分为四部分：" class="headerlink" title="主要分为四部分："></a>主要分为四部分：</h3><h5 id="1，计算距离dists："><a href="#1，计算距离dists：" class="headerlink" title="1，计算距离dists："></a>1，计算距离dists：</h5><p>即每一个测试集都训练集中的数据进行距离计算，得到一个（num_test,num_train）大小的矩阵dists。</p><h5 id="2，设置K，找出前K个最小的距离"><a href="#2，设置K，找出前K个最小的距离" class="headerlink" title="2，设置K，找出前K个最小的距离"></a>2，设置K，找出前K个最小的距离</h5><p>此处利用了np.argsort升序排序并返回索引</p><h5 id="3，统计投票数votes"><a href="#3，统计投票数votes" class="headerlink" title="3，统计投票数votes"></a>3，统计投票数votes</h5><p>①<strong>Python标准库——collections模块的Counter类具体用法就是：</strong></p><p>Counter类的目的是用来跟踪值出现的次数。它是一个无序的容器类型，以字典的键值对形式存储，其中元素作为key，其计数作为value。</p><p>②<strong>Numpy中的用法：</strong></p><p>首先利用np.bincount统计计数，即投票过程，然后利用np.argmax统计最多次数结果作为预测输出</p><p><code>y_pred[i] = np.argmax(np.bincount(closest_y))</code></p><h5 id="4，交叉验证"><a href="#4，交叉验证" class="headerlink" title="4，交叉验证"></a>4，交叉验证</h5><h5 id="5，验证准确率"><a href="#5，验证准确率" class="headerlink" title="5，验证准确率"></a>5，验证准确率</h5><h4 id="最后还有一些jupyter-notebook中的一些简单快捷键和操作命令"><a href="#最后还有一些jupyter-notebook中的一些简单快捷键和操作命令" class="headerlink" title="最后还有一些jupyter notebook中的一些简单快捷键和操作命令"></a><code>最后还有一些jupyter notebook中的一些简单快捷键和操作命令</code></h4><p><code>%run：运行脚本</code></p><p><code>%timeit：测试一行语句执行时间，会将代码运行多次，且会计算平均运行时间</code><br><code>%%timeit：测试语句段的运行时间</code></p><p><code>%time：测试一行执行时间</code><br><code>%%time：测试语段执行时间</code><br><code>程序执行时间：CPU时间（多核情况）时间；Wall时间：实际经过的物理时间</code></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Numpy与KNN学习小结&quot;&gt;&lt;a href=&quot;#Numpy与KNN学习小结&quot; class=&quot;headerlink&quot; title=&quot;Numpy与KNN学习小结&quot;&gt;&lt;/a&gt;Numpy与KNN学习小结&lt;/h1&gt;&lt;p&gt;本周主要借着对cs231n课程作业的对Cifar10
      
    
    </summary>
    
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>博客配置相关</title>
    <link href="http://yoursite.com/2019/04/06/%E5%8D%9A%E5%AE%A2%E9%85%8D%E7%BD%AE%E7%9B%B8%E5%85%B3/"/>
    <id>http://yoursite.com/2019/04/06/博客配置相关/</id>
    <published>2019-04-06T08:58:48.000Z</published>
    <updated>2019-05-11T05:54:29.760Z</updated>
    
    <content type="html"><![CDATA[<p><strong>经常需要调整博客的页面布局，记录一些常用配置以备忘:</strong> </p><blockquote><p>调整hexo页面宽度:<br>博客在浏览器上的留白太多，因此想增加文章的宽度。对于Pisces Scheme，修改页面宽度的方式与其他三个主题不太一样，因此列出Pisces Scheme的修改方式。<br>打开/Hexo/themes/hexo-theme-next/source/css/_variables/custom.styl 添加两行代码即可：</p></blockquote><p><code>$main-desktop = 1200px</code></p><p><code>$content-desktop = 900px</code></p><p>参考官方给出的说明：<br><a href="https://theme-next.iissnan.com/faqs.html" target="_blank" rel="noopener">https://theme-next.iissnan.com/faqs.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;经常需要调整博客的页面布局，记录一些常用配置以备忘:&lt;/strong&gt; &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;调整hexo页面宽度:&lt;br&gt;博客在浏览器上的留白太多，因此想增加文章的宽度。对于Pisces Scheme，修改页面宽度的方式与其他三个主题不
      
    
    </summary>
    
    
    
      <category term="博客配置相关" scheme="http://yoursite.com/tags/%E5%8D%9A%E5%AE%A2%E9%85%8D%E7%BD%AE%E7%9B%B8%E5%85%B3/"/>
    
  </entry>
  
  <entry>
    <title>tf.boys之Tensorflow笔记【三】</title>
    <link href="http://yoursite.com/2019/04/06/tf-boys%E4%B9%8BTensorflow%E7%AC%94%E8%AE%B0%E3%80%90%E4%B8%89%E3%80%91/"/>
    <id>http://yoursite.com/2019/04/06/tf-boys之Tensorflow笔记【三】/</id>
    <published>2019-04-06T06:48:38.000Z</published>
    <updated>2019-04-18T09:50:04.018Z</updated>
    
    <content type="html"><![CDATA[<h1 id="VGG与FCN网络学习"><a href="#VGG与FCN网络学习" class="headerlink" title="VGG与FCN网络学习"></a>VGG与FCN网络学习</h1><p>又到了周末时间了，本周主要将VGG16，19（按照网上说明数字表示其网络的层数）用于分类网络的结构原理给弄明白了，并且按照其网络结构自己敲了一遍VGG16的网络，并且对Cifar10数据进行了分类。感觉过程中遇到一些问题，有的是很小的毛病，有的是很容易犯的问题，有时候遇到问题还是要多去网上查阅一些相关博客或者是官方给出的说明文件，这样可以尽可能地减少走弯路的次数。另外并将FCN用于图像分割的全卷积网络效果给实现了一下，感觉效果不是很好，主要是有几个原因，首先电脑的显卡1050Ti就只有4G显存，训练网络都跑得有点呛【咳咳，早知道当时多花一点钱追求一下需求好了，不过，感觉显卡永远是满足不了需求的：）】</p><h2 id="1-VGG遇到的问题："><a href="#1-VGG遇到的问题：" class="headerlink" title="1.VGG遇到的问题："></a>1.VGG遇到的问题：</h2><h3 id="1-1数据获取及one-hot："><a href="#1-1数据获取及one-hot：" class="headerlink" title="1.1数据获取及one_hot："></a>1.1数据获取及one_hot：</h3><p>由于之前一直用的是mnist，里面已经有封装好的数据读取以及next_batch函数，非常方便，只需要了解其接口函数，直接拿来使用就行了，不用管数据是怎么实现进行读取和操作的，但是如果想自己实现分类一些不仅仅限制于手写字符的图片，那么就需要自己掌握以下几种图像的数据读取方式了。</p><p>我用的是cifar10数据集，但是没用TensorFlow给出的封装形式，自己在网上下载了python版本的batch文件，之后采用了<code>#pickle读取cifar10数据</code>。最后可以返回数据标签和其图像数据。其中由于Cifar10中的数据是以字典形式对应各个标签，其一共10类（1~10），如果在VGG训练过程中利用tf.nn.softmax_cross_entropy计算时需要注意将其转换为one_hot形式，其中TensorFlow给出了<code>tf.one_hot(y,c)</code>以供选择。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">filename=<span class="string">r'F:\pythonwork\VggTest\data\cifar-10-python\cifar-10-batches-py\data_batch_1'</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">labels_data</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="keyword">if</span>  filename:</span><br><span class="line">        <span class="keyword">with</span> open(filename,<span class="string">'rb'</span>) <span class="keyword">as</span> fo:</span><br><span class="line">            data = pickle.load(fo,encoding=<span class="string">'latin1'</span>)</span><br><span class="line">            <span class="comment">#查看一下pickle文件的字典形式</span></span><br><span class="line">            print(data1.keys())</span><br><span class="line">            pictures=data[<span class="string">'data'</span>]</span><br><span class="line">            labels=data[<span class="string">'labels'</span>]</span><br><span class="line">            <span class="keyword">return</span>  labels,pictures</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"The data isn't exist!!!"</span>)</span><br></pre></td></tr></table></figure><h3 id="1-2-训练过程中网络误差损失Loss一直为Nan"><a href="#1-2-训练过程中网络误差损失Loss一直为Nan" class="headerlink" title="1.2.训练过程中网络误差损失Loss一直为Nan"></a>1.2.训练过程中网络误差损失Loss一直为Nan</h3><p>我在网上查阅了相关博客之后，发现是主要由于两个原因：除0或者Log(0)，这两个步骤主要在求交叉熵代价函数时会涉及到，那么，可以根据需要将其避免出现0的情况，可以采用函数：<code>tf.clip_by_value(y,min,max)</code>将Y的值域限制在最小值和最大值之间，从而避免出现0的情况。</p><h3 id="1-3-交叉熵计算函数的封装的计算方法"><a href="#1-3-交叉熵计算函数的封装的计算方法" class="headerlink" title="1.3.交叉熵计算函数的封装的计算方法"></a>1.3.交叉熵计算函数的封装的计算方法</h3><blockquote><p>通常在训练标签数据时，将y表示为网络输出结果，y_表示为标准答案。</p><p>两个方法都已经封装好了softmax，因此直接将网络输出结果代入。</p></blockquote><p>第一种：<code>tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name=None)</code></p><p>labels参数要注意，且在labels中的每个值必须是在[0，num_classes)，它的shape必须是<code>[d_0, d_1, ..., d_{r-1}]&#39;（即batch_size的大小），而参数logits的shape是[d_0, d_1, ..., d_{r-1},num_classes]</code>数据类型必须是int32或者int64，否则在这个操作运行在cpu的时候将会出现exception，运行在GPU的时候将会返回’NaN’,而不是返回loss了，所以在看到输出的不是loss值而是‘NaN’时就应该仔细检查一下这个函数中的labels有没有符合条件）。</p><p>第二种：<code>tf.nn.softmax_cross_entropy_with_logits(logits, labels, name=None)</code></p><p>logits和labels的shape都是[batch_size, num_classes]。数据类型可以是float16 ，<code>float32</code> or <code>float64</code>.），是标签的onehot向量参与计算。softmax_cross_entropy_with_logits 的 labels 是 sparse_softmax_cross_entropy_with_logits 的 labels 的一个独热版本（one hot version）。</p><blockquote><p>以分10类情况，训练时的batch size为batch_num</p><p>（1） 则若使用 softmax_cross_entropy_with_logits， 则其labels参数需要是一个[batch_size, 10]的矩阵，其中每行代表一个instance, 是one hot的形式，其非0 index代表属于哪一类。</p><p>（2）若使用sparse_softmax_cross_entropy_with_logits， 则其Labels参数是一个[batch_size]的列，里面每个属于0到9中间的整数，代表类别，所以函数名称加了sparse, 类似稀疏表示。</p></blockquote><h2 id="2-FCN遇到的问题："><a href="#2-FCN遇到的问题：" class="headerlink" title="2.FCN遇到的问题："></a>2.FCN遇到的问题：</h2><p>在训练时GPU给张量分配空间时总是显示OMM即读取数据时卡死：OOM when allocating tensor with shape[2,151,4096,4096] and type float on/job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc之类的，提示说在分配张量时显卡的内存不足（OOM，即Out of Memory）。</p><h3 id="可以从几个方面考虑："><a href="#可以从几个方面考虑：" class="headerlink" title="可以从几个方面考虑："></a>可以从几个方面考虑：</h3><h4 id="2-1-减小Batch-size训练批次数目（然而我的以及设置为2已经很小了）"><a href="#2-1-减小Batch-size训练批次数目（然而我的以及设置为2已经很小了）" class="headerlink" title="2.1.减小Batch_size训练批次数目（然而我的以及设置为2已经很小了）"></a>2.1.减小Batch_size训练批次数目（然而我的以及设置为2已经很小了）</h4><h4 id="2-2-减小训练图片的尺寸大小，源尺寸可以看见时224-224的（原因时由于经过5次pooling-后得到的图片为原来大小的1-32，这个尺寸的方便处理）但是FCN由于网络的设计是可以输入任意尺寸的图片的。所以可以尽可能将原图片尺寸调小。"><a href="#2-2-减小训练图片的尺寸大小，源尺寸可以看见时224-224的（原因时由于经过5次pooling-后得到的图片为原来大小的1-32，这个尺寸的方便处理）但是FCN由于网络的设计是可以输入任意尺寸的图片的。所以可以尽可能将原图片尺寸调小。" class="headerlink" title="2.2.减小训练图片的尺寸大小，源尺寸可以看见时224*224的（原因时由于经过5次pooling 后得到的图片为原来大小的1/32，这个尺寸的方便处理）但是FCN由于网络的设计是可以输入任意尺寸的图片的。所以可以尽可能将原图片尺寸调小。"></a>2.2.减小训练图片的尺寸大小，源尺寸可以看见时224*224的（原因时由于经过5次pooling 后得到的图片为原来大小的1/32，这个尺寸的方便处理）但是FCN由于网络的设计是可以输入任意尺寸的图片的。所以可以尽可能将原图片尺寸调小。</h4><h4 id="2-3-缩减网络的复杂程度，因为在网络最后会收集参数，最后参数数量以百万级，所以，可以根据自己调整的图片大小，同时调整网络各层输出的特征图数目（即卷积核深度）。"><a href="#2-3-缩减网络的复杂程度，因为在网络最后会收集参数，最后参数数量以百万级，所以，可以根据自己调整的图片大小，同时调整网络各层输出的特征图数目（即卷积核深度）。" class="headerlink" title="2.3.缩减网络的复杂程度，因为在网络最后会收集参数，最后参数数量以百万级，所以，可以根据自己调整的图片大小，同时调整网络各层输出的特征图数目（即卷积核深度）。"></a>2.3.缩减网络的复杂程度，因为在网络最后会收集参数，最后参数数量以百万级，所以，可以根据自己调整的图片大小，同时调整网络各层输出的特征图数目（即卷积核深度）。</h4>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;VGG与FCN网络学习&quot;&gt;&lt;a href=&quot;#VGG与FCN网络学习&quot; class=&quot;headerlink&quot; title=&quot;VGG与FCN网络学习&quot;&gt;&lt;/a&gt;VGG与FCN网络学习&lt;/h1&gt;&lt;p&gt;又到了周末时间了，本周主要将VGG16，19（按照网上说明数字表示其
      
    
    </summary>
    
    
      <category term="tf.boys(Tensorflow)" scheme="http://yoursite.com/categories/tf-boys-Tensorflow/"/>
    
    
      <category term="tf.boys(Tensorflow)笔记" scheme="http://yoursite.com/tags/tf-boys-Tensorflow-%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>tf-boys之Tensorflow笔记【二】</title>
    <link href="http://yoursite.com/2019/03/23/tf-boys%E4%B9%8BTensorflow%E7%AC%94%E8%AE%B0%E3%80%90%E4%BA%8C%E3%80%91/"/>
    <id>http://yoursite.com/2019/03/23/tf-boys之Tensorflow笔记【二】/</id>
    <published>2019-03-23T08:36:49.000Z</published>
    <updated>2019-04-18T09:57:20.876Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Tensorflow之LeNet5"><a href="#Tensorflow之LeNet5" class="headerlink" title="Tensorflow之LeNet5"></a><center>Tensorflow之LeNet5</center></h1><p>今天把LeNet5按照教程敲了一边，中途遇到许多问题，所以记录一下备忘。</p><p><strong>1.要养成根据最后一次callback的位置对错误进行快速定位。</strong></p><p><strong>2.变量名称作用域tf.name_scope()和变量作用域tf.variable_scope()不能写错</strong></p><p>例如：</p><p>​    <code>tf.variable_scope(&#39;layer1-conv1&#39;):</code></p><p>​    <code>tf.name_scope(&#39;layer2-pool1&#39;):</code></p><p>一个是卷积层一个是池化层。<br>常见报错类型：</p><p>​    <code>ValueError: Variable weight already exists, disallowed.</code><br><code>Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:</code></p><p><strong>3.with后面缺少tab键缩进造成变量未被后续程序识别。</strong></p><p>​    <code>ValueError: Variable layer1-conv1/weight already exists, disallowed.</code><br><code>Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:</code></p><p><strong>4.np.reshape和tf.reshape的区别：</strong></p><p>​    <code>TypeError: The value of a feed cannot be a tf.Tensor object.</code><br><code>Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles.</code><br><code>For reference, the tensor object was Tensor(&quot;Reshape:0&quot;, shape=(100, 28, 28, 1), dtype=float32)</code><br><code>which was passed to the feed with key Tensor(&quot;x-inpyt:0&quot;, shape=(100, 28, 28, 1), dtype=float32).</code><br>tensorflow 中的tf.reshape之后不能被feed_dict读入数据操作。</p><p><center><img src="/images/sence1.jpg" alt><center></center></center></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Tensorflow之LeNet5&quot;&gt;&lt;a href=&quot;#Tensorflow之LeNet5&quot; class=&quot;headerlink&quot; title=&quot;Tensorflow之LeNet5&quot;&gt;&lt;/a&gt;&lt;center&gt;Tensorflow之LeNet5&lt;/center&gt;&lt;
      
    
    </summary>
    
    
      <category term="tf.boys(Tensorflow)" scheme="http://yoursite.com/categories/tf-boys-Tensorflow/"/>
    
    
      <category term="tf.boys(Tensorflow)笔记" scheme="http://yoursite.com/tags/tf-boys-Tensorflow-%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>tf-boys之Tensorflow笔记【一】</title>
    <link href="http://yoursite.com/2019/03/22/tf-boys%E4%B9%8BTensorflow%E7%AC%94%E8%AE%B0%E3%80%90%E4%B8%80%E3%80%91/"/>
    <id>http://yoursite.com/2019/03/22/tf-boys之Tensorflow笔记【一】/</id>
    <published>2019-03-22T14:20:33.000Z</published>
    <updated>2019-04-18T09:47:32.240Z</updated>
    
    <content type="html"><![CDATA[<center>Tensorflow学习笔记</center><p>几个函数的用法：</p><p>1.正则化</p><p>​    <code>regularizer=tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)</code></p><p>2.学习率指数衰减</p><p>​    <code>tf.train.exponential_decay(learning_rate,global_,decay_steps,decay_rate,staircase=True/False)</code></p><p>3.移动平均</p><p>​    <code>tf.train.ExponentialMovingAverage</code></p><p>4.将元素添加到列表</p><p>​    <code>tf.add_to_collection(name,value)</code><br>函数将元素添加到列表中</p><p>参数:</p><p>name:列表名,如果不存在,创建一个新的列表<br>value：元素</p><p>5.函数获取列表</p><p>​    <code>tf.get_collection(name)</code></p><p>参数:</p><p>name:列表名</p><p>6.函数将元素相加并返回</p><p>​    <code>tf.add_n(inputs)</code></p><p>注意:元素类型必须一致,否者报错</p><p>​    <code>tf.add_to_collection(‘losses‘, regularizer(weights))</code></p><p>​    <code>tf.add_n(tf.get_collection(‘losses‘))</code></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;center&gt;Tensorflow学习笔记&lt;/center&gt;

&lt;p&gt;几个函数的用法：&lt;/p&gt;
&lt;p&gt;1.正则化&lt;/p&gt;
&lt;p&gt;​    &lt;code&gt;regularizer=tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE
      
    
    </summary>
    
    
      <category term="tf.boys(Tensorflow)" scheme="http://yoursite.com/categories/tf-boys-Tensorflow/"/>
    
    
      <category term="tf.boys(Tensorflow)笔记" scheme="http://yoursite.com/tags/tf-boys-Tensorflow-%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>科技幻想篇之刀剑神域【一】</title>
    <link href="http://yoursite.com/2019/03/22/%E7%A7%91%E6%8A%80%E5%B9%BB%E6%83%B3%E7%AF%87%E4%B9%8B%E5%88%80%E5%89%91%E7%A5%9E%E5%9F%9F%E3%80%90%E4%B8%80%E3%80%91/"/>
    <id>http://yoursite.com/2019/03/22/科技幻想篇之刀剑神域【一】/</id>
    <published>2019-03-22T04:10:00.000Z</published>
    <updated>2019-05-16T05:43:57.444Z</updated>
    
    <content type="html"><![CDATA[<h1 id="科技生活幻想之刀剑神域【一】"><a href="#科技生活幻想之刀剑神域【一】" class="headerlink" title="科技生活幻想之刀剑神域【一】"></a><center>科技生活幻想之刀剑神域【一】</center></h1><p>​    </p><p>今天在知乎上看到一个问题：<a href="https://www.zhihu.com/question/20093001" target="_blank" rel="noopener">怎样才能弄清楚自己想要的是什么？</a>看到下面许多很不错的回答，<strong>寻找答案是一个需要不断刷新认知推倒重来的过程。</strong></p><h2 id="1）你理想中的人生状态是什么样的呢？"><a href="#1）你理想中的人生状态是什么样的呢？" class="headerlink" title="1）你理想中的人生状态是什么样的呢？"></a><strong>1）你理想中的人生状态是什么样的呢？</strong></h2><p>这是一个很直接的问题，说实话或许很多人都没有仔细考虑过，仿佛在读了大学之后，我的人生状态一直就处于一种时而温水煮青蛙，又时而快马加鞭，快到自己都没有看清楚自己对什么到底感兴趣，究竟自己想做的事情是什么？</p><p>看到这个问题之后我回到宿舍，回想了一下，其实自己对于人生理想的状态之前也是会有所考虑的，这时不禁要提要最近在热播的《<em>刀剑神域 第三季Alicization篇</em>》。说起刀剑神域对我影响应该算比较深远也感触比较多的一部动漫了，第一次看应该时2015年，时逢第二季完结，而且剧场版 序列之争也完毕，所以我有幸一口气补完了这之前所有的两季。看完之后就在同时感叹作者的奇思妙想，庞大的世界游戏观以及构造了完全沉浸式头盔和一个高度人工智能的游戏世界以及现实交织，让我对其产生了浓厚的兴趣。</p><div align="center"><br>    <img src="/images/sao3.jpg" alt="刀剑神域第一季"><br>刀剑神域第一季</div><p>所以这之后我就想象（哈哈哈感觉实现起来……）有着自己的科技产品，卖着一些人们的心中可以从中获得温暖的智能产品，可以聊天，问答，喜怒哀乐；现在的年轻一代好像对于孤独和寂寞有着另外一层的理解，或许我们这一代年轻人对于自身的最求远大于对心灵知己的追求，但是事实上每个人都是渴望心中有着那么一个Soulmate。因此，我就想着可以是否可以开发处一种类似刀剑神域中的那种具有高级人工智能的产品，她可以输入记忆输入情感，并且不断增强和学习，而且可以为其输入自己怀念的人的模样那种水晶玻璃杯大小的便于携带的全息影像，陪伴老人和自己。</p><center><img src="/images/sao2.jpg" alt></center><p>同时，我发觉最近这些年来，有关二次元中的合成音，以及合成影像在各种大型活动中亮相的次数也相继在增多，初音未来以及洛天依的语音系统大致是以<a href="https://baike.baidu.com/item/Yamaha" target="_blank" rel="noopener">Yamaha</a>公司的<a href="https://baike.baidu.com/item/VOCALOID3" target="_blank" rel="noopener">VOCALOID</a>系列语音合成引擎为基础利用一些声优的语音采集合成的，在一些演唱会中使用3D全息透明屏幕，只显示来自某一特定角度的图像，而忽略其他角度的光线。即使是在环境光线很亮的地方，也能显示非常明亮、清晰的影像。2010年3月9日世嘉公司举办了一场名为“初音未来日的感谢祭”“初音之日”（Miku’sDay）的初音未来全息投影演唱会。投影机直接背投在全息投影膜，虽然是全息，但不是最理想化的全息投影。</p><center><img src="/images/miku1.png" height="640" width="360"></center><p>因此，在感叹科技发展的同时，我在想自己是否也能够以参与者的身份，为其具体化，将其尺寸设置在一个水晶玻璃杯大小空间，其具有深度学习网络中的记忆系统，可以输入记忆信号，以及语言系统都是可供选择的，以及视觉图像系统都是高度发展的，反应速度和理解能力与人们差不多，而且更中要的是价格是每个人都能够在一定范围内承受得起得。</p><h2 id="2）在理想的人生状态中你自己每天在做什么呢？"><a href="#2）在理想的人生状态中你自己每天在做什么呢？" class="headerlink" title="2）在理想的人生状态中你自己每天在做什么呢？"></a><strong>2）在理想的人生状态中你自己每天在做什么呢？</strong></h2><p>说到自己理想得状态做什么，很多人都会说，那当然是做自己想做的事情了，但是自己仔细想一想我首先会想到在一个空旷的草原躺在上边仰望着天空，时不时可以去旁边的湖边钓钓鱼，当然这在当今的社会中仿佛就是一种极度奢侈的空想，其实仔细想一想，这些事情在小学初中时期其实某种意义上都是已经做过的了，那时候的夏天，无忧无虑，周末有时候和小伙伴们商量该去哪个河边捉螃蟹和钓鱼，早上一大早就出门，一直到下午五点多，弄得一身脏之后，大家一起就跳进河边开始了嬉闹，但是，回到家中当然少不了家人的一顿叨唠。</p><p>在过了大学之后，仿佛，我做什么事情都是必须建立在必须绝对“正确”的角度，没有人会给你任性和放纵的机会了，做任何事情都必须是目的性极强，以结果为导向，而不是做这件事之前就要考虑自己是否感兴趣，是否能够投入为前提。所以，要说自己的人生状态中每天在做什么的话，我想应该是在自己的科技店铺内，每天考虑着怎样创造处一些有趣的科技产品，然后可以真正让别人感到暖心的东西，这些东西就是我所渴望并且希望能够制作出来的一种完全人工智能的情感3D的AI，小巧而且便捷，可以陪伴着每一个孤独的人。然后在忙碌之后，每周有时间去体验儿时那种山水的宁静。可以有说走就走的旅行。</p><h2 id="3）自己生活中有哪些时刻让自己最有成就感？"><a href="#3）自己生活中有哪些时刻让自己最有成就感？" class="headerlink" title="3）自己生活中有哪些时刻让自己最有成就感？"></a><strong>3）自己生活中有哪些时刻让自己最有成就感？</strong></h2><p>生活中最有成就感的时候想必就是努力获得回报的时候吧，具体一点就是自己写的BUG终于运行出来结果的时候应该是最有成就感的时刻……：）</p><hr><p><br></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;科技生活幻想之刀剑神域【一】&quot;&gt;&lt;a href=&quot;#科技生活幻想之刀剑神域【一】&quot; class=&quot;headerlink&quot; title=&quot;科技生活幻想之刀剑神域【一】&quot;&gt;&lt;/a&gt;&lt;center&gt;科技生活幻想之刀剑神域【一】&lt;/center&gt;&lt;/h1&gt;&lt;p&gt;​    
      
    
    </summary>
    
    
      <category term="科技随想" scheme="http://yoursite.com/categories/%E7%A7%91%E6%8A%80%E9%9A%8F%E6%83%B3/"/>
    
    
      <category term="生活感悟" scheme="http://yoursite.com/tags/%E7%94%9F%E6%B4%BB%E6%84%9F%E6%82%9F/"/>
    
  </entry>
  
  <entry>
    <title>开篇博客</title>
    <link href="http://yoursite.com/2019/03/06/%E5%BC%80%E7%AF%87%E5%8D%9A%E5%AE%A2/"/>
    <id>http://yoursite.com/2019/03/06/开篇博客/</id>
    <published>2019-03-06T02:14:45.000Z</published>
    <updated>2019-04-06T13:56:39.666Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言："><a href="#引言：" class="headerlink" title="引言："></a><strong>引言：</strong></h1><p>有一天你将破蛹而出，成长得比人们期待的还要美丽，但这个过程会很痛苦，会很辛苦，有时候还会觉得会心。面对着波涛汹涌的现实觉得自己渺小无力。但是，这也是生命的一部分。做好你现在能做的，一切都会好的。我们都将孤独地长大，不要害怕。</p><p><br></p><p><em>这个年纪的我们，根本不须畏惧有什么会失去，抬起头骄傲的活下去就可以。因为最想要的那个得不到，就看不到自己拥有的一切了，因为最想要的那个得不到，就觉得自己一无所有了。《踮脚张望的时光》寂地</em></p><p><br></p><p><em>人与生俱来的局限是能力与愿望之间永恒的距离，而我们就是在不断跨越的过程中成长。</em>依稀记得这是我高考时候语文作文每次都要用到的一句话哈哈，说起来当时就是在背课文，背名句一点都没错，现在回想起来，这些烙印在记忆深处的话，也是一笔珍贵的回忆和财富呢？<br>​不知不觉在2018年7月份已经读完人生中宝贵的大学时光，目前已经迈向了研究生的生活。回想起来，如同白驹过隙，那些时间仿佛就在眼前，但是却不可及。<br>​不知道是否每个人都会随着年龄的增长，偶尔会回忆起自己前些年的有趣的时光而时不时发出傻笑：)有过欢笑，有过悲伤，有过遗憾。我时常在想自己是否也是一个多愁善感的人呢？答案我自己也不太清楚，因为，感觉随着时光的步伐，自己隐约感觉到许多的朋友和知己如果长时间不联系的话，有时或许都少了一份聊天的话题，然而，我想，这或许是我想多了。曾经看到过一句话：有一种朋友，就算很久没有见面，也不会感到陌生。在人生的旅途中遇到兴趣相同的朋友也是一件值得珍惜的事情。</p><p><br></p><p>去年回家，无意间在整理自己的书籍时，看到了自己小学时期老师让写日记，歪歪扭扭的一页字，寥寥数语，记录的一天最真实，纯真的事情，自己也无意间笑出了声。随着时间的推移，我仿佛不愿将自己的许多事情再如同小学时期那种单纯，许多事情都会藏在心底，只有自己知晓。近来，发觉博客是个符合自己期待的一个工具，可以自己记录所想，所念，同时也可以在必要的时候分享给想要了解的人。同时希望自己能够坚持下去，记录生活，记录感悟。</p><center><img src="/images/saber2.jpg" alt></center><p><br></p><p><br></p><hr><p><br></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;引言：&quot;&gt;&lt;a href=&quot;#引言：&quot; class=&quot;headerlink&quot; title=&quot;引言：&quot;&gt;&lt;/a&gt;&lt;strong&gt;引言：&lt;/strong&gt;&lt;/h1&gt;&lt;p&gt;有一天你将破蛹而出，成长得比人们期待的还要美丽，但这个过程会很痛苦，会很辛苦，有时候还会觉得会心。
      
    
    </summary>
    
    
      <category term="随笔" scheme="http://yoursite.com/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
      <category term="生活感悟" scheme="http://yoursite.com/tags/%E7%94%9F%E6%B4%BB%E6%84%9F%E6%82%9F/"/>
    
  </entry>
  
</feed>
